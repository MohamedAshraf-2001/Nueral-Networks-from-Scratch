{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef273747",
   "metadata": {},
   "source": [
    "## Single Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "accdc58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8]\n"
     ]
    }
   ],
   "source": [
    "inputs = [1.0, 2.0, 3.0, 2.5]\n",
    "weights = [0.2, 0.8, -0.5, 1.0]\n",
    "bias = 2.0\n",
    "\n",
    "output = [\n",
    "    inputs[0]*weights[0] +\n",
    "    inputs[1]*weights[1] +\n",
    "    inputs[2]*weights[2] +\n",
    "    inputs[3]*weights[3] + bias\n",
    "]\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06253a9e",
   "metadata": {},
   "source": [
    "## Layer of Neurons \n",
    "- Neural networks typically have layers that consist of more than one neuron. \n",
    "- Layers are nothing more than groups of neurons. \n",
    "- This is called a fully connected neural network — every neuron in the current layer has connections to every neuron from the previous layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5202f5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8, 1.21, 3.465]\n"
     ]
    }
   ],
   "source": [
    "inputs = [1, 2, 3, 2.5]\n",
    "\n",
    "weights1 = [0.2, 0.8, -0.5, 1]\n",
    "weights2 = [0.5, -0.91, 0.26, -0.5]\n",
    "weights3 = [-0.26, 0.27, 0.17, 0.87]\n",
    "\n",
    "bias1 = 2.0\n",
    "bias2 = 3.0\n",
    "bias3 = 0.5\n",
    "\n",
    "outputs = [\n",
    "    \n",
    "    #Neuron 1:\n",
    "    inputs[0] * weights1[0] +\n",
    "    inputs[1] * weights1[1] +\n",
    "    inputs[2] * weights1[2] +\n",
    "    inputs[3] * weights1[3] + bias1,\n",
    "    \n",
    "    #Neuron 2:\n",
    "    inputs[0] * weights2[0] +\n",
    "    inputs[1] * weights2[1] +\n",
    "    inputs[2] * weights2[2] +\n",
    "    inputs[3] * weights2[3] + bias2,\n",
    "    \n",
    "    #Neuron 3:\n",
    "    inputs[0] * weights3[0] +\n",
    "    inputs[1] * weights3[1] +\n",
    "    inputs[2] * weights3[2] +\n",
    "    inputs[3] * weights3[3] + bias3\n",
    "]\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7c493c",
   "metadata": {},
   "source": [
    "## A Single Neuron with NumPy\n",
    "This makes the code much simpler to read and write (and faster to run):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e36d329a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "inputs = [1.0, 2.0, 3.0, 2.5]\n",
    "weights = [0.2, 0.8, -0.5, 1.0]\n",
    "bias = 2.0\n",
    "\n",
    "outputs = np.dot(weights, inputs) + bias \n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d0b98f",
   "metadata": {},
   "source": [
    "## A Layer of Neurons with NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "069880b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8   1.21  3.465]\n",
      "(3, 4) x  (4,) = (3,)\n"
     ]
    }
   ],
   "source": [
    "inputs = np.array([1.0, 2.0, 3.0, 2.5])\n",
    "weights = np.array([\n",
    "    [0.2, 0.8, -0.5, 1],\n",
    "    [0.5, -0.91, 0.26, -0.5],\n",
    "    [-0.26, 0.27, 0.17, 0.87]\n",
    "])\n",
    "\n",
    "biases = np.array([2.0, 3.0, 0.5])\n",
    "\n",
    "outputs = np.dot(weights, inputs) + biases\n",
    "\n",
    "print(outputs)\n",
    "print('{} x  {} = {}'.format(weights.shape, inputs.shape, outputs.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e4851f",
   "metadata": {},
   "source": [
    "## A Layer of Neurons & Batch of Data NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9bf3d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of inputs:  (3, 4)\n",
      "shape of weights:  (3, 4)\n",
      "outputs:  [[ 4.8    1.21   2.385]\n",
      " [ 8.9   -1.81   0.2  ]\n",
      " [ 1.41   1.051  0.026]]\n"
     ]
    }
   ],
   "source": [
    "inputs = np.array([\n",
    "    [1.0, 2.0, 3.0, 2.5],\n",
    "    [2.0, 5.0, -1.0, 2.0],\n",
    "    [-1.5, 2.7, 3.3, -0.8]\n",
    "])\n",
    "\n",
    "weights = np.array([\n",
    "    [0.2, 0.8, -0.5, 1.0],\n",
    "    [0.5, -0.91, 0.26, -0.5],\n",
    "    [-0.26, -0.27, 0.17, 0.87]\n",
    "])\n",
    "\n",
    "biases = np.array([2.0, 3.0, 0.5])\n",
    "\n",
    "print('shape of inputs: ',inputs.shape)\n",
    "print('shape of weights: ',weights.shape)\n",
    "\n",
    "outputs = np.dot(inputs, weights.T) + biases\n",
    "print('outputs: ', outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bf64b1",
   "metadata": {},
   "source": [
    "## Adding Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ac7d426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of inputs:  (3, 4)\n",
      "shape of weights in first hidden layer:  (3, 4)\n",
      "shape of weights in second hidden layer:  (3, 3)\n",
      "shape of layer1 outputs (3, 3)\n",
      "shape of layer2 outputs (3, 3)\n",
      "Outputs:  [[-2.1744   3.21425  1.19065]\n",
      " [ 0.707    0.6828   4.5213 ]\n",
      " [-1.39594  1.9477  -0.14521]]\n"
     ]
    }
   ],
   "source": [
    "inputs = np.array([\n",
    "    [1.0, 2.0, 3.0, 2.5],\n",
    "    [2.0, 5.0, -1.0, 2.0],\n",
    "    [-1.5, 2.7, 3.3, -0.8]\n",
    "])\n",
    "\n",
    "weights1 = np.array([\n",
    "    [0.2, 0.8, -0.5, 1.0],\n",
    "    [0.5, -0.91, 0.26, -0.5],\n",
    "    [-0.26, -0.27, 0.17, 0.87]\n",
    "])  #4 Neurons\n",
    "biases1 = np.array([2.0, 3.0, 0.5])\n",
    "\n",
    "weights2 = np.array(\n",
    "    [[0.1, -0.14, 0.5],\n",
    "    [-0.5, 0.12, -0.33],\n",
    "    [-0.44, 0.73, -0.13]    \n",
    "])  #3 Neurons\n",
    "biases2 = np.array([-1, 2, -0.5])\n",
    "\n",
    "print('shape of inputs: ',inputs.shape)\n",
    "print('shape of weights in first hidden layer: ',weights1.shape)\n",
    "print('shape of weights in second hidden layer: ',weights2.shape)\n",
    "\n",
    "layer1_outputs = np.dot(inputs, weights1.T) + biases1\n",
    "print('shape of layer1 outputs', layer1_outputs.shape)\n",
    "layer2_outputs = np.dot(layer1_outputs, weights2) + biases2\n",
    "print('shape of layer2 outputs', layer2_outputs.shape)\n",
    "\n",
    "print('Outputs: ', layer2_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a6fe62",
   "metadata": {},
   "source": [
    "# Activation Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a2ed99",
   "metadata": {},
   "source": [
    "## Sigmoid Function\n",
    "The sigmoid activation function is a mathematical function commonly used in artificial neural networks to introduce non-linearity into the output of a neuron. It takes any input value and transforms it into a value between 0 and 1.\n",
    "\n",
    "The formula for the sigmoid function is:\n",
    "\n",
    "**f(x) = 1 / (1 + e^-x)**\n",
    "\n",
    "where x is the input to the function, e is the mathematical constant Euler's number (approximately 2.71828), and f(x) is the output of the function.\n",
    "\n",
    "The graph of the sigmoid function is an S-shaped curve, where the output values start near 0 when the input is very negative, gradually increase to 0.5 at x=0, and then continue to increase towards 1 as the input becomes positive.\n",
    "\n",
    "The sigmoid function is often used as an activation function in the output layer of binary classification problems, where the goal is to predict one of two classes. It can also be used in the hidden layers of a neural network to introduce non-linearity into the model. However, the sigmoid function can suffer from the vanishing gradient problem, which can make it difficult to train deep neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e9ca487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5619131e",
   "metadata": {},
   "source": [
    "## Rectified Linear Unit (ReLU) Function:\n",
    "As mentioned earlier, with “dead neurons,” it’s usually better to have a more granular approach for the hidden neuron activation functions.\n",
    "The Sigmoid function, historically used in hidden layers, was eventually replaced by the Rectified Linear Units activation function (or ReLU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ad534a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(inputs):\n",
    "    return np.maximum(0, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2dc302",
   "metadata": {},
   "source": [
    "## Softmax Function\n",
    "The softmax activation function is a mathematical function used in machine learning and deep learning for classification tasks. It takes as input a vector of real-valued numbers and transforms them into a probability distribution, where each element in the output vector represents the probability of a particular class.\n",
    "\n",
    "The softmax function is defined as follows:\n",
    "\n",
    "**softmax(x) = e^x / sum(e^x)**\n",
    "\n",
    "where x is a vector of real numbers, e is the mathematical constant approximately equal to 2.71828, and sum(e^x) is the sum of the exponential of each element in the input vector.\n",
    "\n",
    "The output of the softmax function is a probability distribution, where each element in the output vector represents the probability of a particular class. The sum of all the probabilities in the output vector is equal to 1.\n",
    "\n",
    "The softmax function is commonly used as the output activation function in neural networks for classification tasks. It is particularly useful when the number of classes is greater than two, as it allows the neural network to assign probabilities to each possible class, rather than simply choosing between two options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "249eafc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Softmax(x):\n",
    "    e_x = np.exp(inputs - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "49de5e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.26313249, 0.04329137, 0.42226619, 0.60848946],\n",
       "       [0.71526828, 0.8695305 , 0.00773408, 0.36906752],\n",
       "       [0.02159923, 0.08717812, 0.56999974, 0.02244302]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Softmax(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ad4b0a",
   "metadata": {},
   "source": [
    "# FULL NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff038b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.weights1 = np.random.randn(self.input_size, self.hidden_size) * 0.01\n",
    "        self.weights2 = np.random.randn(self.hidden_size, self.output_size) * 0.01\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Forward pass\n",
    "        self.z1 = np.dot(X, self.weights1)\n",
    "        self.a1 = self.sigmoid(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.weights2)\n",
    "        y_pred = self.sigmoid(self.z2)\n",
    "        return y_pred\n",
    "    \n",
    "    def backward(self, X, y, y_pred, learning_rate):\n",
    "        # Backward pass\n",
    "        delta2 = (y_pred - y) * self.sigmoid_derivative(self.z2)\n",
    "        d_weights2 = np.dot(self.a1.T, delta2)\n",
    "        delta1 = np.dot(delta2, self.weights2.T) * self.sigmoid_derivative(self.z1)\n",
    "        d_weights1 = np.dot(X.T, delta1)\n",
    "        \n",
    "        # Update weights\n",
    "        self.weights1 -= learning_rate * d_weights1\n",
    "        self.weights2 -= learning_rate * d_weights2\n",
    "    \n",
    "    def train(self, X, y, learning_rate, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            y_pred = self.forward(X)\n",
    "            \n",
    "            # Backward pass\n",
    "            self.backward(X, y, y_pred, learning_rate)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = np.mean(np.square(y_pred - y))\n",
    "            \n",
    "            # Print loss every 100 epochs\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}: Loss = {loss:.4f}\")\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def sigmoid_derivative(self, z):\n",
    "        return self.sigmoid(z) * (1 - self.sigmoid(z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ad1fc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 0.2500\n",
      "Epoch 100: Loss = 0.2500\n",
      "Epoch 200: Loss = 0.2500\n",
      "Epoch 300: Loss = 0.2500\n",
      "Epoch 400: Loss = 0.2500\n",
      "Epoch 500: Loss = 0.2500\n",
      "Epoch 600: Loss = 0.2500\n",
      "Epoch 700: Loss = 0.2500\n",
      "Epoch 800: Loss = 0.2500\n",
      "Epoch 900: Loss = 0.2500\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=1)\n",
    "nn.train(X, y, learning_rate=0.1, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57beae76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
